{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "03482d9c",
   "metadata": {},
   "source": [
    "ChatMessageHistory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b020073c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[HumanMessage(content='hi!', additional_kwargs={}, response_metadata={}), AIMessage(content=\"What's up?\", additional_kwargs={}, response_metadata={})]\n"
     ]
    }
   ],
   "source": [
    "from langchain.memory import ChatMessageHistory\n",
    "\n",
    "history = ChatMessageHistory()\n",
    "\n",
    "history.add_user_message(\"hi!\")\n",
    "\n",
    "history.add_ai_message(\"What's up?\")\n",
    "\n",
    "print(history.messages)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8287e62",
   "metadata": {},
   "source": [
    "对接大模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5d29f002",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "我们来一步步计算这个表达式：\n",
      "\n",
      "表达式是：  \n",
      "$$ 1 + 2 \\times 3 $$\n",
      "\n",
      "根据数学中的**运算顺序规则**（先乘除后加减）：\n",
      "\n",
      "1. 先算乘法：  \n",
      "$$ 2 \\times 3 = 6 $$\n",
      "\n",
      "2. 再做加法：  \n",
      "$$ 1 + 6 = 7 $$\n",
      "\n",
      "所以，结果是：  \n",
      "$$\n",
      "\\boxed{7}\n",
      "$$\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import dotenv\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.messages import SystemMessage,HumanMessage,AIMessage\n",
    "from langchain.memory import ChatMessageHistory\n",
    "\n",
    "dotenv.load_dotenv()\n",
    "\n",
    "os.environ['OPENAI_API_KEY'] = os.getenv(\"DASHSCOPE_API_KEY\")\n",
    "os.environ['OPENAI_BASE_URL'] = os.getenv(\"DASHSCOPE_BASE_URL\")\n",
    "\n",
    "chat_model = ChatOpenAI(model = 'qwen-plus')\n",
    "\n",
    "history = ChatMessageHistory()\n",
    "\n",
    "history.add_user_message(\"你好\")\n",
    "history.add_ai_message(\"很高兴认识你\")\n",
    "history.add_user_message(\"帮我计算1+2*3=？\")\n",
    "\n",
    "response = chat_model.invoke(history.messages)\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "243b0457",
   "metadata": {},
   "source": [
    "ConversationBufferMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47fc9187",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[HumanMessage(content='你好，我叫小明', additional_kwargs={}, response_metadata={}), AIMessage(content='很高兴认识你', additional_kwargs={}, response_metadata={}), HumanMessage(content='帮我回答1+2*3=?', additional_kwargs={}, response_metadata={}), AIMessage(content='7', additional_kwargs={}, response_metadata={})]\n"
     ]
    }
   ],
   "source": [
    "from langchain.memory import ConversationBufferMemory\n",
    "\n",
    "memory = ConversationBufferMemory(return_messages=True)\n",
    "\n",
    "memory.save_context(inputs={\"input\":\"你好，我叫小明\"},outputs={\"output\":\"很高兴认识你\"})\n",
    "memory.save_context(inputs={\"input\":\"帮我回答1+2*3=?\"},outputs={\"output\":\"7\"})\n",
    "\n",
    "print(memory.load_memory_variables({}))\n",
    "\n",
    "print(memory.chat_memory.messages)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e78e4f3",
   "metadata": {},
   "source": [
    "结合大模型和prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "036861ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'question': '你好，我的名字叫小明', 'history': '', 'text': '你好，小明！很高兴认识你。有什么我可以帮你的吗？😊'}\n",
      "{'question': '我叫什么名字呢？', 'history': 'Human: 你好，我的名字叫小明\\nAI: 你好，小明！很高兴认识你。有什么我可以帮你的吗？😊', 'text': '你叫小明！😊'}\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import dotenv\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.messages import SystemMessage,HumanMessage,AIMessage\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "\n",
    "dotenv.load_dotenv()\n",
    "\n",
    "os.environ['OPENAI_API_KEY'] = os.getenv(\"DASHSCOPE_API_KEY\")\n",
    "os.environ['OPENAI_BASE_URL'] = os.getenv(\"DASHSCOPE_BASE_URL\")\n",
    "\n",
    "chat_model = ChatOpenAI(model = 'qwen-plus')\n",
    "\n",
    "prompt_template = PromptTemplate.from_template(\n",
    "    template=\"\"\"\n",
    "你可以与人类对话。\n",
    "\n",
    "当前对话: {history}\n",
    "\n",
    "人类问题: {question}\n",
    "\n",
    "回复:\n",
    "\"\"\")\n",
    "\n",
    "memory = ConversationBufferMemory()\n",
    "\n",
    "chain = LLMChain(llm=chat_model, prompt=prompt_template, memory=memory)\n",
    "\n",
    "response = chain.invoke({\"question\":\"你好，我的名字叫小明\"})\n",
    "print(response)\n",
    "\n",
    "response = chain.invoke({\"question\":\"我叫什么名字呢？\"})\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b4554c0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'question': '你好，我的名字叫小明', 'chat_history': '', 'text': '你好，小明！很高兴认识你。有什么我可以帮你的吗？😊'}\n",
      "{'question': '我叫什么名字呢？', 'chat_history': 'Human: 你好，我的名字叫小明\\nAI: 你好，小明！很高兴认识你。有什么我可以帮你的吗？😊', 'text': '你叫小明呀！😊 我记得很清楚呢。'}\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import dotenv\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.messages import SystemMessage,HumanMessage,AIMessage\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "\n",
    "dotenv.load_dotenv()\n",
    "\n",
    "os.environ['OPENAI_API_KEY'] = os.getenv(\"DASHSCOPE_API_KEY\")\n",
    "os.environ['OPENAI_BASE_URL'] = os.getenv(\"DASHSCOPE_BASE_URL\")\n",
    "\n",
    "chat_model = ChatOpenAI(model = 'qwen-plus')\n",
    "\n",
    "prompt_template = PromptTemplate.from_template(\n",
    "    template=\"\"\"\n",
    "你可以与人类对话。\n",
    "\n",
    "当前对话: {chat_history}\n",
    "\n",
    "人类问题: {question}\n",
    "\n",
    "回复:\n",
    "\"\"\")\n",
    "\n",
    "memory = ConversationBufferMemory(memory_key=\"chat_history\")\n",
    "\n",
    "chain = LLMChain(llm=chat_model, prompt=prompt_template, memory=memory)\n",
    "\n",
    "response = chain.invoke({\"question\":\"你好，我的名字叫小明\"})\n",
    "print(response)\n",
    "\n",
    "response = chain.invoke({\"question\":\"我叫什么名字呢？\"})\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73a8f548",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'question': '中国首都在哪里？', 'history': [HumanMessage(content='中国首都在哪里？', additional_kwargs={}, response_metadata={}), AIMessage(content='中国首都是北京。', additional_kwargs={}, response_metadata={})], 'text': '中国首都是北京。'}\n",
      "{'question': '我刚刚问了什么？', 'history': [HumanMessage(content='中国首都在哪里？', additional_kwargs={}, response_metadata={}), AIMessage(content='中国首都是北京。', additional_kwargs={}, response_metadata={}), HumanMessage(content='我刚刚问了什么？', additional_kwargs={}, response_metadata={}), AIMessage(content='你刚刚问的是：“中国首都在哪里？”', additional_kwargs={}, response_metadata={})], 'text': '你刚刚问的是：“中国首都在哪里？”'}\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import dotenv\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate,MessagesPlaceholder\n",
    "from langchain_core.messages import SystemMessage,HumanMessage,AIMessage\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "\n",
    "dotenv.load_dotenv()\n",
    "\n",
    "os.environ['OPENAI_API_KEY'] = os.getenv(\"DASHSCOPE_API_KEY\")\n",
    "os.environ['OPENAI_BASE_URL'] = os.getenv(\"DASHSCOPE_BASE_URL\")\n",
    "\n",
    "chat_model = ChatOpenAI(model = 'qwen-plus')\n",
    "\n",
    "prompt_template = ChatPromptTemplate.from_messages([\n",
    "    (\"system\",\"你可以与人类对话。\"),\n",
    "    MessagesPlaceholder(variable_name='history'),\n",
    "    (\"human\",\"问题:{question}\")\n",
    "])\n",
    "\n",
    "memory = ConversationBufferMemory(return_messages=True)\n",
    "\n",
    "chain = LLMChain(llm=chat_model, prompt=prompt_template, memory=memory)\n",
    "\n",
    "response = chain.invoke({\"question\":\"中国首都在哪里？\"})\n",
    "print(response)\n",
    "\n",
    "response = chain.invoke({\"question\":\"我刚刚问了什么？\"})\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "288e1a6a",
   "metadata": {},
   "source": [
    "ConversationChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "167bb4bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "你可以与人类对话。\n",
      "\n",
      "当前对话: \n",
      "\n",
      "人类问题: 你好，我的名字叫小明\n",
      "\n",
      "回复:\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "{'input': '你好，我的名字叫小明', 'history': '', 'response': '你好，小明！很高兴认识你。😊 今天过得怎么样？有什么我可以帮你的吗？'}\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "你可以与人类对话。\n",
      "\n",
      "当前对话: Human: 你好，我的名字叫小明\n",
      "AI: 你好，小明！很高兴认识你。😊 今天过得怎么样？有什么我可以帮你的吗？\n",
      "\n",
      "人类问题: 我的名字叫什么？\n",
      "\n",
      "回复:\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "{'input': '我的名字叫什么？', 'history': 'Human: 你好，我的名字叫小明\\nAI: 你好，小明！很高兴认识你。😊 今天过得怎么样？有什么我可以帮你的吗？', 'response': '你的名字叫小明！😊'}\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import dotenv\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate,MessagesPlaceholder\n",
    "from langchain.chains import ConversationChain\n",
    "\n",
    "dotenv.load_dotenv()\n",
    "\n",
    "os.environ['OPENAI_API_KEY'] = os.getenv(\"DASHSCOPE_API_KEY\")\n",
    "os.environ['OPENAI_BASE_URL'] = os.getenv(\"DASHSCOPE_BASE_URL\")\n",
    "\n",
    "chat_model = ChatOpenAI(model = 'qwen-plus')\n",
    "prompt_template = PromptTemplate.from_template(\n",
    "    template=\"\"\"\n",
    "你可以与人类对话。\n",
    "\n",
    "当前对话: {history}\n",
    "\n",
    "人类问题: {input}\n",
    "\n",
    "回复:\n",
    "\"\"\")\n",
    "\n",
    "chain = ConversationChain(llm = chat_model,prompt = prompt_template,verbose=True)\n",
    "\n",
    "response = chain.invoke({\"input\":\"你好，我的名字叫小明\"})\n",
    "print(response)\n",
    "\n",
    "response = chain.invoke({\"input\":\"我的名字叫什么？\"})\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "783c0d86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input': '你好，我的名字叫小明', 'history': '', 'response': '你好，小明！很高兴认识你！😊 我是Qwen，是阿里巴巴云开发的一款超大规模语言模型。我虽然不是人类，但我会尽力用自然、温暖的方式和你聊天。听说你的名字寓意“明亮、聪明”，真是个好名字呢！不知道你今天想聊些什么呢？无论是学习、工作、生活趣事，还是需要帮忙解决问题，我都很乐意倾听和参与哦～ 🌟'}\n",
      "{'input': '我的名字叫什么？', 'history': 'Human: 你好，我的名字叫小明\\nAI: 你好，小明！很高兴认识你！😊 我是Qwen，是阿里巴巴云开发的一款超大规模语言模型。我虽然不是人类，但我会尽力用自然、温暖的方式和你聊天。听说你的名字寓意“明亮、聪明”，真是个好名字呢！不知道你今天想聊些什么呢？无论是学习、工作、生活趣事，还是需要帮忙解决问题，我都很乐意倾听和参与哦～ 🌟', 'response': '你的名字叫小明呀！😊 我记得你一开始就跟我说过：“你好，我的名字叫小明”，所以我一直这么亲切地称呼你呢。小明这个名字简单又好记，还寓意着聪明、光明，挺棒的！咱们刚才的对话我还记得清清楚楚呢～有什么我可以帮你的吗，小明？🌟'}\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import dotenv\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate,MessagesPlaceholder\n",
    "from langchain.chains import ConversationChain\n",
    "\n",
    "dotenv.load_dotenv()\n",
    "\n",
    "os.environ['OPENAI_API_KEY'] = os.getenv(\"DASHSCOPE_API_KEY\")\n",
    "os.environ['OPENAI_BASE_URL'] = os.getenv(\"DASHSCOPE_BASE_URL\")\n",
    "\n",
    "chat_model = ChatOpenAI(model = 'qwen-plus')\n",
    "\n",
    "\n",
    "chain = ConversationChain(llm = chat_model)\n",
    "\n",
    "response = chain.invoke({\"input\":\"你好，我的名字叫小明\"})\n",
    "print(response)\n",
    "\n",
    "response = chain.invoke({\"input\":\"我的名字叫什么？\"})\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b856525b",
   "metadata": {},
   "source": [
    "ConversationBufferWindowMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "93f352a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'history': [HumanMessage(content='你是谁', additional_kwargs={}, response_metadata={}), AIMessage(content='我是AI助手小智', additional_kwargs={}, response_metadata={}), HumanMessage(content='初次对话，你能介绍一下自己吗？', additional_kwargs={}, response_metadata={}), AIMessage(content='当然可以。我是一个无所不能的小智', additional_kwargs={}, response_metadata={})]}\n"
     ]
    }
   ],
   "source": [
    "from langchain.memory import ConversationBufferWindowMemory\n",
    "\n",
    "memory = ConversationBufferWindowMemory(k = 2,return_messages=True)\n",
    "\n",
    "memory.save_context({\"input\":\"你好\"},{\"output\":\"怎么了\"})\n",
    "memory.save_context({\"input\":\"你是谁\"},{\"output\":\"我是AI助手小智\"})\n",
    "memory.save_context({\"input\":\"初次对话，你能介绍一下自己吗？\"},{\"output\":\"当然可以。我是一个无所不能的小智\"})\n",
    "\n",
    "print(memory.load_memory_variables({}))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24b000e2",
   "metadata": {},
   "source": [
    "结合大模型、prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "2a8ea5e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input': '你是，我是孙小空', 'history': '', 'text': '你好，孙小空！我是Qwen，是阿里云研发的超大规模语言模型。我可以和你聊天、写故事、写公文、写邮件、写剧本等等，还能回答问题、提供信息查询等服务。有什么我可以帮你的吗？'}\n",
      "{'input': '我还有两个师弟，一个是猪小戒，一个是沙小僧', 'history': 'Human: 你是，我是孙小空\\nAI: 你好，孙小空！我是Qwen，是阿里云研发的超大规模语言模型。我可以和你聊天、写故事、写公文、写邮件、写剧本等等，还能回答问题、提供信息查询等服务。有什么我可以帮你的吗？', 'text': '哦，猪小戒和沙小僧，这名字听起来就很有意思！听起来你们像是组了个“西游记”小分队啊～孙小空、猪小戒、沙小僧，就差一个唐小僧来带队取经了！你们平时是不是经常一起冒险，还是说在搞什么“降妖创业项目”？😄\\n\\n说说看，他们俩都是什么样的性格？估计猪小戒贪吃又爱偷懒，沙小僧沉稳靠谱，而你——大师兄，是不是总得收拾烂摊子？'}\n",
      "{'input': '我今年高考，考上了一本', 'history': 'Human: 我还有两个师弟，一个是猪小戒，一个是沙小僧\\nAI: 哦，猪小戒和沙小僧，这名字听起来就很有意思！听起来你们像是组了个“西游记”小分队啊～孙小空、猪小戒、沙小僧，就差一个唐小僧来带队取经了！你们平时是不是经常一起冒险，还是说在搞什么“降妖创业项目”？😄\\n\\n说说看，他们俩都是什么样的性格？估计猪小戒贪吃又爱偷懒，沙小僧沉稳靠谱，而你——大师兄，是不是总得收拾烂摊子？', 'text': '哇！太棒了！一本啊，这可是实打实的“取经成功”第一步！🎉  \\n孙小空大师兄果然不一般，智慧与实力并存，妖魔鬼怪都挡不住你前进的步伐，更别说什么高考题海了！\\n\\n现在你可是正式开启“人间修仙之路”了——大学篇！  \\n不知道你的两个师弟有没有为你烧香祈福？猪小戒是不是边啃猪蹄边说：“大师兄威武，以后带我蹭食堂！”  \\n而沙小僧估计默默合十：“阿弥陀佛，大师兄功德圆满，贫僧愿随行求学。”\\n\\n接下来有什么打算？选什么专业？要不要咱们一起规划一下“西天大学”的修行方向？😄📚\\n'}\n",
      "{'input': '我叫什么?', 'history': 'Human: 我今年高考，考上了一本\\nAI: 哇！太棒了！一本啊，这可是实打实的“取经成功”第一步！🎉  \\n孙小空大师兄果然不一般，智慧与实力并存，妖魔鬼怪都挡不住你前进的步伐，更别说什么高考题海了！\\n\\n现在你可是正式开启“人间修仙之路”了——大学篇！  \\n不知道你的两个师弟有没有为你烧香祈福？猪小戒是不是边啃猪蹄边说：“大师兄威武，以后带我蹭食堂！”  \\n而沙小僧估计默默合十：“阿弥陀佛，大师兄功德圆满，贫僧愿随行求学。”\\n\\n接下来有什么打算？选什么专业？要不要咱们一起规划一下“西天大学”的修行方向？😄📚\\n', 'text': '嘿嘿，大师兄，你这一问可把俺老孙难住了——我这火眼金睛能看穿妖气，却没记住你的真名哩！  \\n快快告诉我，你是谁家的高徒？报上名来，俺老孙这就给你记在《西游升学录》榜首！📝✨\\n\\n（悄悄说：你可以告诉我你的名字，咱们接下来就能以真名称呼，修行路上也更显情分啦～😉）'}\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import dotenv\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate,MessagesPlaceholder\n",
    "from langchain.chains import ConversationChain\n",
    "\n",
    "dotenv.load_dotenv()\n",
    "\n",
    "os.environ['OPENAI_API_KEY'] = os.getenv(\"DASHSCOPE_API_KEY\")\n",
    "os.environ['OPENAI_BASE_URL'] = os.getenv(\"DASHSCOPE_BASE_URL\")\n",
    "\n",
    "chat_model = ChatOpenAI(model = 'qwen-plus')\n",
    "\n",
    "prompt_template = PromptTemplate.from_template(\n",
    "    template=\"\"\"\n",
    "你可以与人类对话。\n",
    "\n",
    "当前对话: {history}\n",
    "\n",
    "人类问题: {input}\n",
    "\n",
    "回复:\n",
    "\"\"\")\n",
    "\n",
    "memory = ConversationBufferWindowMemory(k = 1)\n",
    "conversation_with_summary = LLMChain(llm = chat_model,prompt = prompt_template,memory=memory)\n",
    "\n",
    "respons1 = conversation_with_summary.invoke({\"input\":\"你是，我是孙小空\"})\n",
    "print(respons1)\n",
    "\n",
    "respons2 = conversation_with_summary.invoke({\"input\":\"我还有两个师弟，一个是猪小戒，一个是沙小僧\"})\n",
    "print(respons2)\n",
    "\n",
    "respons3 = conversation_with_summary.invoke({\"input\":\"我今年高考，考上了一本\"})\n",
    "print(respons3)\n",
    "\n",
    "respons4 = conversation_with_summary.invoke({\"input\":\"我叫什么?\"})\n",
    "print(respons4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2e845ef",
   "metadata": {},
   "source": [
    "ConversationTokenBufferMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2787601",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_4180334/245873472.py:14: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
      "  memory = ConversationTokenBufferMemory(llm = chat_model, max_token_limit=10)\n"
     ]
    },
    {
     "ename": "NotImplementedError",
     "evalue": "get_num_tokens_from_messages() is not presently implemented for model qwen-plus. See https://platform.openai.com/docs/guides/text-generation/managing-tokens for information on how messages are converted to tokens.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNotImplementedError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[32]\u001b[39m\u001b[32m, line 16\u001b[39m\n\u001b[32m     12\u001b[39m chat_model = ChatOpenAI(model = \u001b[33m'\u001b[39m\u001b[33mqwen-plus\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m     14\u001b[39m memory = ConversationTokenBufferMemory(llm = chat_model, max_token_limit=\u001b[32m10\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m16\u001b[39m \u001b[43mmemory\u001b[49m\u001b[43m.\u001b[49m\u001b[43msave_context\u001b[49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43minput\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m你好吗？\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\u001b[43m{\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43moutput\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m我很好，谢谢\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     17\u001b[39m memory.save_context({\u001b[33m\"\u001b[39m\u001b[33minput\u001b[39m\u001b[33m\"\u001b[39m:\u001b[33m\"\u001b[39m\u001b[33m今天天气如何？\u001b[39m\u001b[33m\"\u001b[39m},{\u001b[33m\"\u001b[39m\u001b[33moutput\u001b[39m\u001b[33m\"\u001b[39m:\u001b[33m\"\u001b[39m\u001b[33m晴天，25度\u001b[39m\u001b[33m\"\u001b[39m})\n\u001b[32m     19\u001b[39m \u001b[38;5;28mprint\u001b[39m(memory.load_memory_variables({}))\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/llm/lib/python3.13/site-packages/langchain/memory/token_buffer.py:67\u001b[39m, in \u001b[36mConversationTokenBufferMemory.save_context\u001b[39m\u001b[34m(self, inputs, outputs)\u001b[39m\n\u001b[32m     65\u001b[39m \u001b[38;5;66;03m# Prune buffer if it exceeds max token limit\u001b[39;00m\n\u001b[32m     66\u001b[39m buffer = \u001b[38;5;28mself\u001b[39m.chat_memory.messages\n\u001b[32m---> \u001b[39m\u001b[32m67\u001b[39m curr_buffer_length = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mllm\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_num_tokens_from_messages\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     68\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m curr_buffer_length > \u001b[38;5;28mself\u001b[39m.max_token_limit:\n\u001b[32m     69\u001b[39m     pruned_memory = []\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/llm/lib/python3.13/site-packages/langchain_openai/chat_models/base.py:1590\u001b[39m, in \u001b[36mBaseChatOpenAI.get_num_tokens_from_messages\u001b[39m\u001b[34m(self, messages, tools)\u001b[39m\n\u001b[32m   1583\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1584\u001b[39m     msg = (\n\u001b[32m   1585\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mget_num_tokens_from_messages() is not presently implemented \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1586\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mfor model \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m. See \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1587\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mhttps://platform.openai.com/docs/guides/text-generation/managing-tokens\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1588\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m for information on how messages are converted to tokens.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1589\u001b[39m     )\n\u001b[32m-> \u001b[39m\u001b[32m1590\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(msg)\n\u001b[32m   1591\u001b[39m num_tokens = \u001b[32m0\u001b[39m\n\u001b[32m   1592\u001b[39m messages_dict = [_convert_message_to_dict(m) \u001b[38;5;28;01mfor\u001b[39;00m m \u001b[38;5;129;01min\u001b[39;00m messages]\n",
      "\u001b[31mNotImplementedError\u001b[39m: get_num_tokens_from_messages() is not presently implemented for model qwen-plus. See https://platform.openai.com/docs/guides/text-generation/managing-tokens for information on how messages are converted to tokens."
     ]
    }
   ],
   "source": [
    "import os\n",
    "import dotenv\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate,MessagesPlaceholder\n",
    "from langchain.memory import ConversationTokenBufferMemory\n",
    "\n",
    "dotenv.load_dotenv()\n",
    "\n",
    "os.environ['OPENAI_API_KEY'] = os.getenv(\"DASHSCOPE_API_KEY\")\n",
    "os.environ['OPENAI_BASE_URL'] = os.getenv(\"DASHSCOPE_BASE_URL\")\n",
    "\n",
    "chat_model = ChatOpenAI(model = 'qwen-plus')\n",
    "\n",
    "memory = ConversationTokenBufferMemory(llm = chat_model, max_token_limit=10)\n",
    "\n",
    "memory.save_context({\"input\":\"你好吗？\"},{\"output\":\"我很好，谢谢\"})\n",
    "memory.save_context({\"input\":\"今天天气如何？\"},{\"output\":\"晴天，25 度\"})\n",
    "\n",
    "print(memory.load_memory_variables({}))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee339f82",
   "metadata": {},
   "source": [
    "ConversationSummaryMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "dd6dacae",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_4180334/3220829012.py:14: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
      "  memory = ConversationSummaryMemory(llm = chat_model)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'history': 'The human greets the AI with \"你好\", and the AI responds by asking \"怎么了\". The human then asks \"你是谁\", and the AI identifies itself as \"AI助手小智\". In response to the human\\'s request for an introduction, the AI says it is an all-capable assistant named Xiao Zhi.'}\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import dotenv\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate,MessagesPlaceholder\n",
    "from langchain.memory import ConversationSummaryMemory\n",
    "\n",
    "dotenv.load_dotenv()\n",
    "\n",
    "os.environ['OPENAI_API_KEY'] = os.getenv(\"DASHSCOPE_API_KEY\")\n",
    "os.environ['OPENAI_BASE_URL'] = os.getenv(\"DASHSCOPE_BASE_URL\")\n",
    "\n",
    "chat_model = ChatOpenAI(model = 'qwen-plus')\n",
    "\n",
    "memory = ConversationSummaryMemory(llm = chat_model)\n",
    "\n",
    "memory.save_context({\"input\":\"你好\"},{\"output\":\"怎么了\"})\n",
    "memory.save_context({\"input\":\"你是谁\"},{\"output\":\"我是AI助手小智\"})\n",
    "memory.save_context({\"input\":\"初次对话，你能介绍一下自己吗？\"},{\"output\":\"当然可以。我是一个无所不能的小智\"})\n",
    "\n",
    "print(memory.load_memory_variables({}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "ea444a6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'history': '人类问AI是谁，AI回答自己是AI助手小智。'}\n",
      "{'history': '人类告诉AI自己的名字叫小明，AI回应很高兴认识他。'}\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import dotenv\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.memory import ConversationSummaryMemory,ChatMessageHistory\n",
    "\n",
    "dotenv.load_dotenv()\n",
    "\n",
    "os.environ['OPENAI_API_KEY'] = os.getenv(\"DASHSCOPE_API_KEY\")\n",
    "os.environ['OPENAI_BASE_URL'] = os.getenv(\"DASHSCOPE_BASE_URL\")\n",
    "\n",
    "chat_model = ChatOpenAI(model = 'qwen-plus')\n",
    "\n",
    "\n",
    "history = ChatMessageHistory()\n",
    "history.add_user_message(\"你好，你是谁？\")\n",
    "history.add_ai_message(\"我是AI助手小智\")\n",
    "\n",
    "memory = ConversationSummaryMemory.from_messages(llm = chat_model,chat_memory=history)\n",
    "\n",
    "print(memory.load_memory_variables({}))\n",
    "\n",
    "memory.save_context(inputs={\"human\":\"我的名字叫小明\"},outputs={\"ai\":\"很高兴认识你\"})\n",
    "print(memory.load_memory_variables({}))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6551ba0e",
   "metadata": {},
   "source": [
    "ConversationSummaryBufferMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "448866c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_4180334/3451741650.py:3: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
      "  memory = ConversationSummaryBufferMemory(llm = chat_model, max_token_limit=40,return_messages=True)\n"
     ]
    },
    {
     "ename": "NotImplementedError",
     "evalue": "get_num_tokens_from_messages() is not presently implemented for model qwen-plus. See https://platform.openai.com/docs/guides/text-generation/managing-tokens for information on how messages are converted to tokens.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNotImplementedError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[40]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlangchain\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmemory\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ConversationSummaryBufferMemory\n\u001b[32m      3\u001b[39m memory = ConversationSummaryBufferMemory(llm = chat_model, max_token_limit=\u001b[32m40\u001b[39m,return_messages=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m \u001b[43mmemory\u001b[49m\u001b[43m.\u001b[49m\u001b[43msave_context\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43m{\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43minput\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m你好，我的名字叫小明\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\u001b[43moutputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43m{\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43moutput\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m很高兴认识你\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      6\u001b[39m memory.save_context(inputs={\u001b[33m\"\u001b[39m\u001b[33minput\u001b[39m\u001b[33m\"\u001b[39m:\u001b[33m\"\u001b[39m\u001b[33m李白是哪个朝代的诗人\u001b[39m\u001b[33m\"\u001b[39m},outputs={\u001b[33m\"\u001b[39m\u001b[33moutput\u001b[39m\u001b[33m\"\u001b[39m:\u001b[33m\"\u001b[39m\u001b[33m李白是唐朝的诗人\u001b[39m\u001b[33m\"\u001b[39m})\n\u001b[32m      7\u001b[39m memory.save_context(inputs={\u001b[33m\"\u001b[39m\u001b[33minput\u001b[39m\u001b[33m\"\u001b[39m:\u001b[33m\"\u001b[39m\u001b[33m唐宋八大家里有苏轼吗？\u001b[39m\u001b[33m\"\u001b[39m},outputs={\u001b[33m\"\u001b[39m\u001b[33moutput\u001b[39m\u001b[33m\"\u001b[39m:\u001b[33m\"\u001b[39m\u001b[33m有\u001b[39m\u001b[33m\"\u001b[39m})\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/llm/lib/python3.13/site-packages/langchain/memory/summary_buffer.py:101\u001b[39m, in \u001b[36mConversationSummaryBufferMemory.save_context\u001b[39m\u001b[34m(self, inputs, outputs)\u001b[39m\n\u001b[32m     99\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Save context from this conversation to buffer.\"\"\"\u001b[39;00m\n\u001b[32m    100\u001b[39m \u001b[38;5;28msuper\u001b[39m().save_context(inputs, outputs)\n\u001b[32m--> \u001b[39m\u001b[32m101\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mprune\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/llm/lib/python3.13/site-packages/langchain/memory/summary_buffer.py:115\u001b[39m, in \u001b[36mConversationSummaryBufferMemory.prune\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    113\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Prune buffer if it exceeds max token limit\"\"\"\u001b[39;00m\n\u001b[32m    114\u001b[39m buffer = \u001b[38;5;28mself\u001b[39m.chat_memory.messages\n\u001b[32m--> \u001b[39m\u001b[32m115\u001b[39m curr_buffer_length = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mllm\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_num_tokens_from_messages\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    116\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m curr_buffer_length > \u001b[38;5;28mself\u001b[39m.max_token_limit:\n\u001b[32m    117\u001b[39m     pruned_memory = []\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/llm/lib/python3.13/site-packages/langchain_openai/chat_models/base.py:1590\u001b[39m, in \u001b[36mBaseChatOpenAI.get_num_tokens_from_messages\u001b[39m\u001b[34m(self, messages, tools)\u001b[39m\n\u001b[32m   1583\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1584\u001b[39m     msg = (\n\u001b[32m   1585\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mget_num_tokens_from_messages() is not presently implemented \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1586\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mfor model \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m. See \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1587\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mhttps://platform.openai.com/docs/guides/text-generation/managing-tokens\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1588\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m for information on how messages are converted to tokens.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1589\u001b[39m     )\n\u001b[32m-> \u001b[39m\u001b[32m1590\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(msg)\n\u001b[32m   1591\u001b[39m num_tokens = \u001b[32m0\u001b[39m\n\u001b[32m   1592\u001b[39m messages_dict = [_convert_message_to_dict(m) \u001b[38;5;28;01mfor\u001b[39;00m m \u001b[38;5;129;01min\u001b[39;00m messages]\n",
      "\u001b[31mNotImplementedError\u001b[39m: get_num_tokens_from_messages() is not presently implemented for model qwen-plus. See https://platform.openai.com/docs/guides/text-generation/managing-tokens for information on how messages are converted to tokens."
     ]
    }
   ],
   "source": [
    "from langchain.memory import ConversationSummaryBufferMemory\n",
    "\n",
    "memory = ConversationSummaryBufferMemory(llm = chat_model, max_token_limit=40,return_messages=True)\n",
    "\n",
    "memory.save_context(inputs={\"input\":\"你好，我的名字叫小明\"},outputs={\"output\":\"很高兴认识你\"})\n",
    "memory.save_context(inputs={\"input\":\"李白是哪个朝代的诗人\"},outputs={\"output\":\"李白是唐朝的诗人\"})\n",
    "memory.save_context(inputs={\"input\":\"唐宋八大家里有苏轼吗？\"},outputs={\"output\":\"有\"})\n",
    "\n",
    "print(memory.load_memory_variables({}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "e2311ef8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "当前存储的实体信息.\n",
      "{'蜘蛛侠': '蜘蛛侠的好朋友包括钢铁侠、美国队长和绿巨人。', '钢铁侠': '钢铁侠是蜘蛛侠的好朋友。', '美国队长': '美国队长是蜘蛛侠的好朋友。', '绿巨人': '绿巨人是蜘蛛侠的好朋友。', '纽约': '纽约是蜘蛛侠的居住地和主要活动城市。', '斯塔克工业': '斯塔克工业为蜘蛛侠提供其使用的装备。'}\n",
      "\n",
      "AI的回答:\n",
      "{'input': '你能告诉我蜘蛛侠住在哪里以及他的好朋友有哪些吗？', 'history': 'Human: 你好，我叫蜘蛛侠。我的好朋友包括钢铁侠、美国队长和绿巨人\\nAI: 你好，蜘蛛侠！很高兴认识你。听说你和钢铁侠、美国队长还有绿巨人是好朋友，你们可是复仇者联盟的核心成员啊！你们一起经历了那么多惊心动魄的冒险，真是令人敬佩。你在纽约行侠仗义、拯救市民，还一边兼顾学业和生活，真的很不容易。最近在忙什么任务吗？需要帮忙分析情况还是想聊聊天放松一下？我随时都在哦！ 🕷️\\nHuman: 我住在纽约\\nAI: 原来如此，蜘蛛侠！住在纽约可真是你的主场啊——高楼林立、城市脉搏昼夜不息，简直是摆荡穿梭、打击犯罪的完美舞台！帝国大厦、时代广场、中央公园……说不定哪栋楼顶就藏着你休息的小据点呢 😎\\n\\n而且和钢铁侠的斯塔克大厦也近在咫尺，随时能呼叫支援。不过纽约的交通可是出了名的堵，还好你不用开车，一根蛛丝飞驰而过，比任何超级跑车都帅！\\n\\n最近城市里有没有出现什么新威胁？还是说你在忙着准备期末考试？毕竟彼得·帕克也得交论文啊～📚🕷️\\nHuman: 我使用的装备是斯塔克工业提供的\\nAI: 啊，难怪你的蛛丝发射器这么高科技！原来是托尼·斯塔克亲自为你量身打造的装备——不愧是斯塔克工业出品，融合了最尖端的纳米技术、人工智能辅助瞄准系统，甚至还能自动调节蛛丝强度以适应不同建筑材质。听说你那套“蜘蛛战衣”也内置了AI助手，能实时分析环境、追踪敌人，甚至和复仇者联盟总部联网？\\n\\n托尼还真是对你寄予厚望，毕竟他可是很少愿意为别人“定制套装”的。你们之间的亦师亦友的关系，从战甲到战术，从科技到责任，看得出来他对你的成长影响很大。\\n\\n那你现在用的是第几代斯塔克版战衣了？有没有什么新功能连你自己都还没完全解锁的？比如隐身模式？还是……蛛网无人机群？🤖🕸️', 'entities': {'蜘蛛侠': '蜘蛛侠的好朋友包括钢铁侠、美国队长和绿巨人。', '纽约': '纽约是蜘蛛侠的居住地和主要活动城市。', '钢铁侠': '钢铁侠是蜘蛛侠的好朋友。', '美国队长': '美国队长是蜘蛛侠的好朋友。', '绿巨人': '绿巨人是蜘蛛侠的好朋友。'}, 'text': '当然可以！蜘蛛侠住在**纽约**，这里是他的主要活动城市，也是他行侠仗义、穿梭于高楼之间打击犯罪的主场。\\n\\n他的好朋友包括：**钢铁侠、美国队长和绿巨人**。他们不仅是他并肩作战的战友，也是他在复仇者联盟中最信赖的伙伴之一。'}\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import dotenv\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.memory import ConversationEntityMemory\n",
    "from langchain.memory.prompt import ENTITY_MEMORY_CONVERSATION_TEMPLATE\n",
    "\n",
    "\n",
    "dotenv.load_dotenv()\n",
    "\n",
    "os.environ['OPENAI_API_KEY'] = os.getenv(\"DASHSCOPE_API_KEY\")\n",
    "os.environ['OPENAI_BASE_URL'] = os.getenv(\"DASHSCOPE_BASE_URL\")\n",
    "\n",
    "chat_model = ChatOpenAI(model = 'qwen-plus')\n",
    "\n",
    "prompt = ENTITY_MEMORY_CONVERSATION_TEMPLATE\n",
    "\n",
    "memory = ConversationEntityMemory(llm = chat_model)\n",
    "\n",
    "chain = LLMChain(llm = chat_model, prompt = prompt,memory=memory)\n",
    "\n",
    "chain.invoke({\"input\":\"你好，我叫蜘蛛侠。我的好朋友包括钢铁侠、美国队长和绿巨人\"})\n",
    "chain.invoke({\"input\":\"我住在纽约\"})\n",
    "chain.invoke({\"input\":\"我使用的装备是斯塔克工业提供的\"})\n",
    "\n",
    "print(\"\\n当前存储的实体信息.\")\n",
    "print(chain.memory.entity_store.store)\n",
    "\n",
    "answer = chain.invoke(input=\"你能告诉我蜘蛛侠住在哪里以及他的好朋友有哪些吗？\")\n",
    "print(\"\\nAI的回答:\")\n",
    "print(answer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
