{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7bd0f164",
   "metadata": {},
   "source": [
    "1、获取大模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9b1fd0ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"id\":\"chatcmpl-6fbc300c-30d9-47a6-ab35-1721fbbfa56d\",\"choices\":[{\"finish_reason\":\"stop\",\"index\":0,\"logprobs\":null,\"message\":{\"content\":\"我是通义千问，由阿里云研发的超大规模语言模型。我能够回答问题、创作文字，如写故事、公文、邮件、剧本等，还能进行逻辑推理、编程，表达观点，玩游戏等。我支持多种语言，包括但不限于中文、英文、德语、法语、西班牙语等。\\n\\n如果您有任何问题或需要帮助，欢迎随时告诉我！\",\"refusal\":null,\"role\":\"assistant\",\"annotations\":null,\"audio\":null,\"function_call\":null,\"tool_calls\":null}}],\"created\":1760266169,\"model\":\"qwen-plus\",\"object\":\"chat.completion\",\"service_tier\":null,\"system_fingerprint\":null,\"usage\":{\"completion_tokens\":81,\"prompt_tokens\":22,\"total_tokens\":103,\"completion_tokens_details\":null,\"prompt_tokens_details\":{\"audio_tokens\":null,\"cached_tokens\":0}}}\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from openai import OpenAI\n",
    "\n",
    "\n",
    "client = OpenAI(\n",
    "    # 若没有配置环境变量，请用百炼API Key将下行替换为：api_key=\"sk-xxx\",\n",
    "    api_key=\"sk-2ab14110bd284acea0486386523da3c5\",\n",
    "    base_url=\"https://dashscope.aliyuncs.com/compatible-mode/v1\",\n",
    ")\n",
    "\n",
    "completion = client.chat.completions.create(\n",
    "    # 模型列表：https://help.aliyun.com/zh/model-studio/getting-started/models\n",
    "    model=\"qwen-plus\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "        {\"role\": \"user\", \"content\": \"你是谁？\"},\n",
    "    ],\n",
    "    # Qwen3模型通过enable_thinking参数控制思考过程（开源版默认True，商业版默认False）\n",
    "    # 使用Qwen3开源版模型时，若未启用流式输出，请将下行取消注释，否则会报错\n",
    "    # extra_body={\"enable_thinking\": False},\n",
    ")\n",
    "print(completion.model_dump_json())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14ed3809",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "“大模型”是近年来人工智能领域中的一个热门术语，通常指的是**参数量巨大、训练数据庞大、计算资源需求高的深度学习模型**，尤其是在自然语言处理（NLP）领域广泛应用的**大规模预训练语言模型**。\n",
      "\n",
      "### 一、大模型的核心特征\n",
      "\n",
      "1. **参数量巨大**\n",
      "   - 大模型通常拥有数十亿（billion）甚至数千亿（trillion）个可训练参数。\n",
      "   - 例如：GPT-3 有约 1750 亿参数，而 GPT-4 的参数更多（具体未公开）。\n",
      "   - 参数越多，模型的表达能力和记忆容量越强。\n",
      "\n",
      "2. **海量训练数据**\n",
      "   - 使用互联网规模的文本数据进行训练，如网页、书籍、新闻、代码等。\n",
      "   - 数据量可达数TB甚至PB级别。\n",
      "\n",
      "3. **强大的泛化能力**\n",
      "   - 能够在没有专门微调的情况下完成多种任务（零样本或少样本学习），比如翻译、问答、写作、编程等。\n",
      "\n",
      "4. **高计算资源需求**\n",
      "   - 训练大模型需要高性能GPU/TPU集群和大量电力。\n",
      "   - 推理（使用模型）也需要较强的算力支持。\n",
      "\n",
      "---\n",
      "\n",
      "### 二、典型的大模型举例\n",
      "\n",
      "| 模型 | 开发者 | 参数量级 | 特点 |\n",
      "|------|--------|----------|------|\n",
      "| GPT-3 / GPT-3.5 / GPT-4 | OpenAI | 数百亿到数千亿 | 强大的文本生成能力，支持对话、写作等 |\n",
      "| PaLM / Gemini | Google | 5400亿+ | 多模态、多语言能力强 |\n",
      "| LLaMA / LLaMA2 / LLaMA3 | Meta (Facebook) | 7B 到 600B+ | 开源，推动社区发展 |\n",
      "| Qwen（通义千问） | 阿里巴巴 | 最高达千亿级 | 支持中文优化 |\n",
      "| ERNIE Bot | 百度 | 未公开 | 中文语境优化 |\n",
      "| ChatGLM | 智谱AI | 几百亿 | 开源且适合中文场景 |\n",
      "\n",
      "---\n",
      "\n",
      "### 三、大模型的工作方式\n",
      "\n",
      "大模型通常是基于**Transformer架构**构建的，通过以下两个阶段工作：\n",
      "\n",
      "1. **预训练（Pre-training）**\n",
      "   - 在大规模无标注数据上进行自监督学习，学习语言规律。\n",
      "   - 例如：预测下一个词（语言建模任务）。\n",
      "\n",
      "2. **微调或提示工程（Fine-tuning / Prompting）**\n",
      "   - 使用少量标注数据对特定任务进行微调。\n",
      "   - 或通过“提示词”（Prompt）引导模型输出期望结果（如“请写一篇关于气候变化的文章”）。\n",
      "\n",
      "---\n",
      "\n",
      "### 四、大模型的应用场景\n",
      "\n",
      "- 智能客服与聊天机器人（如ChatGPT）\n",
      "- 内容创作（写文章、诗歌、广告文案）\n",
      "- 编程辅助（GitHub Copilot）\n",
      "- 教育辅导（解题、讲解知识点）\n",
      "- 多语言翻译\n",
      "- 医疗咨询辅助\n",
      "- 金融分析与报告生成\n",
      "\n",
      "---\n",
      "\n",
      "### 五、大模型的挑战与问题\n",
      "\n",
      "1. **成本高昂**：训练一次可能花费数百万美元。\n",
      "2. **能耗大**：训练过程碳排放较高。\n",
      "3. **幻觉问题**：会“一本正经地胡说八道”，生成看似合理但错误的内容。\n",
      "4. **偏见与伦理风险**：可能继承训练数据中的偏见。\n",
      "5. **安全与滥用风险**：可用于生成虚假信息、诈骗内容等。\n",
      "\n",
      "---\n",
      "\n",
      "### 六、总结\n",
      "\n",
      "> **大模型 = 大参数 + 大数据 + 大算力 + 强智能**\n",
      "\n",
      "它代表了当前人工智能发展的前沿方向，正在深刻改变人机交互方式和各行各业的运作模式。虽然存在挑战，但其潜力巨大，被认为是通往通用人工智能（AGI）的重要路径之一。\n",
      "\n",
      "如果你感兴趣，我也可以介绍如何入门大模型开发或使用。\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import dotenv\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# 1️⃣ 读取 .env 文件\n",
    "dotenv.load_dotenv()\n",
    "\n",
    "# 2️⃣ 设置环境变量\n",
    "os.environ[\"OPENAI_API_KEY\"] = os.getenv(\"DASHSCOPE_API_KEY\")  # 通义API Key\n",
    "os.environ[\"OPENAI_BASE_URL\"] = \"https://dashscope.aliyuncs.com/compatible-mode/v1\"\n",
    "\n",
    "# 3️⃣ 创建模型实例（Qwen）\n",
    "llm = ChatOpenAI(\n",
    "    model=\"qwen-plus\"\n",
    ")\n",
    "\n",
    "# 4️⃣ 调用模型\n",
    "response = llm.invoke(\"什么是大模型？\")\n",
    "print(response.content)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7553b5c4",
   "metadata": {},
   "source": [
    "2、使用提示词模板"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "db2d1cd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='LangChain 是一个开源的、专为构建基于大语言模型（LLM, Large Language Model）的应用程序而设计的开发框架。它极大地简化了与语言模型集成和交互的过程，使开发者能够高效地构建复杂的自然语言处理应用，如聊天机器人、智能代理（Agent）、文档问答系统、自动化工作流等。\\n\\n虽然名字中带有“Chain”，但 LangChain 并非指区块链技术，而是强调“链式调用”——将多个组件（如提示词、模型调用、外部工具、记忆模块等）串联起来，形成一条“思维链”或“执行链”，从而实现更高级的认知功能。\\n\\n---\\n\\n### 一、LangChain 的核心理念\\n\\nLangChain 的设计哲学是：**让大模型“思考、规划、行动、记忆”**。为此，它提供了一套模块化、可组合的架构，支持以下能力：\\n\\n1. **链式流程（Chains）**  \\n   将多个步骤（如输入处理、调用 LLM、调用工具、后处理）组织成一个有序流程。\\n\\n2. **上下文记忆（Memory）**  \\n   支持在对话中保留历史信息，实现多轮对话的记忆管理。\\n\\n3. **外部工具集成（Tools & Agents）**  \\n   允许 LLM 调用外部 API、数据库、搜索引擎、Python 函数等，扩展其能力边界。\\n\\n4. **数据感知（Data-Aware）**  \\n   可将私有文档、知识库等数据与 LLM 结合，实现基于特定知识的回答（如 RAG 架构）。\\n\\n5. **模块化设计**  \\n   所有组件（Prompt Templates、LLMs、Retrievers、Agents 等）均可独立使用或组合。\\n\\n---\\n\\n### 二、LangChain 的核心组件\\n\\n| 组件 | 功能说明 |\\n|------|----------|\\n| **Models** | 支持多种 LLM（如 OpenAI GPT、Anthropic Claude、Hugging Face 模型等）和嵌入模型（Embeddings）。 |\\n| **Prompts** | 提供提示词模板（Prompt Templates）、示例选择器（Example Selectors），便于动态生成高质量输入。 |\\n| **Chains** | 将多个步骤组合成流程，例如“先总结再翻译”、“先检索再回答”。 |\\n| **Memory** | 管理对话状态，支持短期记忆（如 ConversationBufferMemory）和长期记忆（向量存储）。 |\\n| **Retrievers** | 从文档、数据库或向量数据库中检索相关信息，用于增强 LLM 输出的准确性。 |\\n| **Agents** | 允许 LLM 根据用户请求自主决定调用哪些工具（如搜索、计算、API 调用），实现“智能代理”。 |\\n| **Callbacks** | 提供日志、监控、追踪等功能，便于调试和性能分析。 |\\n\\n---\\n\\n### 三、典型应用场景\\n\\n1. **文档问答系统（RAG）**  \\n   用户上传 PDF 或文本，系统自动提取内容并构建索引，通过 LangChain 实现“基于文档的问答”。\\n\\n2. **智能客服机器人**  \\n   结合记忆模块和业务 API，实现多轮对话与订单查询、退换货处理等任务。\\n\\n3. **AI Agent（智能体）**  \\n   让模型自主决策：例如，“帮我查明天北京天气，并推荐合适的穿衣搭配”，模型会自动调用天气 API 和知识库。\\n\\n4. **自动化工作流**  \\n   将多个 Chain 组合成复杂流程，如“读取邮件 → 分类 → 生成回复草稿 → 发送确认”。\\n\\n5. **代码生成与解释**  \\n   利用 LLM 解释代码逻辑，或根据自然语言描述生成 Python/SQL 脚本。\\n\\n---\\n\\n### 四、LangChain 的优势\\n\\n- ✅ **降低开发门槛**：无需从零实现提示工程、上下文管理、工具调用等复杂逻辑。\\n- ✅ **高度可扩展**：支持自定义组件，适配各种 LLM 和数据源。\\n- ✅ **生态系统丰富**：与 Pinecone、Weaviate、Supabase、Hugging Face 等主流工具无缝集成。\\n- ✅ **活跃社区**：GitHub 星标超 30k，文档完善，案例丰富。\\n\\n---\\n\\n### 五、简单代码示例（Python）\\n\\n```python\\nfrom langchain_openai import ChatOpenAI\\nfrom langchain_core.prompts import ChatPromptTemplate\\nfrom langchain_core.output_parsers import StrOutputParser\\n\\n# 初始化模型\\nllm = ChatOpenAI(model=\"gpt-3.5-turbo\")\\n\\n# 定义提示模板\\nprompt = ChatPromptTemplate.from_template(\"请用中文解释什么是{topic}？\")\\n\\n# 构建链：提示 → 模型 → 解析输出\\nchain = prompt | llm | StrOutputParser()\\n\\n# 调用\\nresponse = chain.invoke({\"topic\": \"人工智能\"})\\nprint(response)\\n```\\n\\n---\\n\\n### 六、注意事项\\n\\n- LangChain 更新频繁，建议关注官方文档（[https://python.langchain.com](https://python.langchain.com)）以获取最新 API。\\n- 对于生产环境，需注意成本控制、延迟优化、安全性和可观测性。\\n- 推荐结合向量数据库（如 Chroma、Pinecone）和 RAG 架构提升准确率。\\n\\n---\\n\\n### 总结\\n\\n> **LangChain 是大模型时代的“操作系统级”开发框架**，它把 LLM 从“玩具”变成“工具”，让开发者能快速构建具备记忆、推理、行动能力的 AI 应用。无论是初创公司还是大型企业，LangChain 都已成为构建 LLM 应用的事实标准之一。\\n\\n如果你正在开发基于大模型的应用，掌握 LangChain 将极大提升开发效率和系统能力。' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 1252, 'prompt_tokens': 27, 'total_tokens': 1279, 'completion_tokens_details': None, 'prompt_tokens_details': {'audio_tokens': None, 'cached_tokens': 0}}, 'model_name': 'qwen-plus', 'system_fingerprint': None, 'id': 'chatcmpl-9209ed8c-a535-45ed-a615-b004cd58c33a', 'service_tier': None, 'finish_reason': 'stop', 'logprobs': None} id='run--aa0aba5d-f227-412c-87a9-32540a30d660-0' usage_metadata={'input_tokens': 27, 'output_tokens': 1252, 'total_tokens': 1279, 'input_token_details': {'cache_read': 0}, 'output_token_details': {}}\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\",\"你是世界级的技术文档编写者\"),\n",
    "    (\"user\", \"{input}\")\n",
    "])\n",
    "\n",
    "chain = prompt | llm\n",
    "message = chain.invoke({\"input\": \"大模型中的LangChain是什么?\"})\n",
    "print(message)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "337d06dd",
   "metadata": {},
   "source": [
    "3、使用输出解析器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "11a3a987",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'question': 'Langchain是什么？',\n",
       " 'answer': 'LangChain 是一个开源框架，旨在简化和加速基于大型语言模型（LLM）的应用程序开发。它提供了一套模块化工具，用于连接语言模型与外部数据源、构建上下文感知的对话系统、实现链式调用（chaining）逻辑、记忆管理以及代理（agent）行为。LangChain 支持多种集成，如向量数据库、文档加载器、工具调用和提示模板管理，广泛应用于聊天机器人、自动化助手、问答系统等场景。其核心概念包括 Chains、Agents、Memory 和 Prompts，帮助开发者灵活构建复杂的 LLM 驱动应用。'}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser,JsonOutputParser\n",
    "\n",
    "llm = ChatOpenAI(model=\"qwen-plus\")\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\",\"你是世界级的技术文档编写者\"),\n",
    "    (\"user\", \"{input}\")\n",
    "])\n",
    "\n",
    "output_parser = JsonOutputParser()\n",
    "\n",
    "chain = prompt | llm | output_parser\n",
    "\n",
    "chain.invoke({\"input\":\"Langchain是什么？用JSON格式回复，问题用question，回答用answer\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a5409f0",
   "metadata": {},
   "source": [
    "4、使用向量存储"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "257e1fa0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mIndexError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[30]\u001b[39m\u001b[32m, line 21\u001b[39m\n\u001b[32m     18\u001b[39m documents = text_splitter.split_documents(docs)\n\u001b[32m     19\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mlen\u001b[39m(documents))\n\u001b[32m---> \u001b[39m\u001b[32m21\u001b[39m vector=\u001b[43mFAISS\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_documents\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdocuments\u001b[49m\u001b[43m,\u001b[49m\u001b[43membeddings\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/llm/lib/python3.13/site-packages/langchain_core/vectorstores/base.py:837\u001b[39m, in \u001b[36mVectorStore.from_documents\u001b[39m\u001b[34m(cls, documents, embedding, **kwargs)\u001b[39m\n\u001b[32m    834\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28many\u001b[39m(ids):\n\u001b[32m    835\u001b[39m         kwargs[\u001b[33m\"\u001b[39m\u001b[33mids\u001b[39m\u001b[33m\"\u001b[39m] = ids\n\u001b[32m--> \u001b[39m\u001b[32m837\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfrom_texts\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtexts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membedding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadatas\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmetadatas\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/llm/lib/python3.13/site-packages/langchain_community/vectorstores/faiss.py:1044\u001b[39m, in \u001b[36mFAISS.from_texts\u001b[39m\u001b[34m(cls, texts, embedding, metadatas, ids, **kwargs)\u001b[39m\n\u001b[32m   1025\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Construct FAISS wrapper from raw documents.\u001b[39;00m\n\u001b[32m   1026\u001b[39m \n\u001b[32m   1027\u001b[39m \u001b[33;03mThis is a user friendly interface that:\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   1041\u001b[39m \u001b[33;03m        faiss = FAISS.from_texts(texts, embeddings)\u001b[39;00m\n\u001b[32m   1042\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   1043\u001b[39m embeddings = embedding.embed_documents(texts)\n\u001b[32m-> \u001b[39m\u001b[32m1044\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m__from\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1045\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtexts\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1046\u001b[39m \u001b[43m    \u001b[49m\u001b[43membeddings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1047\u001b[39m \u001b[43m    \u001b[49m\u001b[43membedding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1048\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmetadatas\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmetadatas\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1049\u001b[39m \u001b[43m    \u001b[49m\u001b[43mids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1050\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1051\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/llm/lib/python3.13/site-packages/langchain_community/vectorstores/faiss.py:1001\u001b[39m, in \u001b[36mFAISS.__from\u001b[39m\u001b[34m(cls, texts, embeddings, embedding, metadatas, ids, normalize_L2, distance_strategy, **kwargs)\u001b[39m\n\u001b[32m    998\u001b[39m     index = faiss.IndexFlatIP(\u001b[38;5;28mlen\u001b[39m(embeddings[\u001b[32m0\u001b[39m]))\n\u001b[32m    999\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1000\u001b[39m     \u001b[38;5;66;03m# Default to L2, currently other metric types not initialized.\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1001\u001b[39m     index = faiss.IndexFlatL2(\u001b[38;5;28mlen\u001b[39m(\u001b[43membeddings\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m))\n\u001b[32m   1002\u001b[39m docstore = kwargs.pop(\u001b[33m\"\u001b[39m\u001b[33mdocstore\u001b[39m\u001b[33m\"\u001b[39m, InMemoryDocstore())\n\u001b[32m   1003\u001b[39m index_to_docstore_id = kwargs.pop(\u001b[33m\"\u001b[39m\u001b[33mindex_to_docstore_id\u001b[39m\u001b[33m\"\u001b[39m, {})\n",
      "\u001b[31mIndexError\u001b[39m: list index out of range"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "import bs4\n",
    "\n",
    "loader = WebBaseLoader(\n",
    "    web_path=\"http://www.xinhuanet.com/politics/2020-12/31/c_1126934011.htm\",\n",
    "    bs_kwargs=dict(parse_only=bs4.SoupStrainer(\"div\", class_=\"article\"))\n",
    ")\n",
    "docs = loader.load()\n",
    "\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-ada-002\")\n",
    "\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)\n",
    "documents = text_splitter.split_documents(docs)\n",
    "print(len(documents))\n",
    "\n",
    "vector=FAISS.from_documents(documents,embeddings)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
