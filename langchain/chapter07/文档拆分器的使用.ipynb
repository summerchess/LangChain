{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6866189d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "å—1:é•¿åº¦:50\n",
      "LangChain æ˜¯ä¸€ä¸ªç”¨äºå¼€å‘ç”±è¯­è¨€æ¨¡å‹é©±åŠ¨çš„åº”ç”¨ç¨‹åºçš„æ¡†æ¶çš„ã€‚å®ƒæä¾›äº†ä¸€å¥—å·¥å…·å’ŒæŠ½è±¡ï¼Œä½¿å¼€å‘è€…\n",
      "--------------------------------------------------\n",
      "å—2:é•¿åº¦:21\n",
      "ï¼Œä½¿å¼€å‘è€…èƒ½å¤Ÿæ›´å®¹æ˜“åœ°æ„å»ºå¤æ‚çš„åº”ç”¨ç¨‹åºã€‚\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from langchain_text_splitters import CharacterTextSplitter\n",
    "\n",
    "text = \"LangChain æ˜¯ä¸€ä¸ªç”¨äºå¼€å‘ç”±è¯­è¨€æ¨¡å‹é©±åŠ¨çš„åº”ç”¨ç¨‹åºçš„æ¡†æ¶çš„ã€‚å®ƒæä¾›äº†ä¸€å¥—å·¥å…·å’ŒæŠ½è±¡ï¼Œä½¿å¼€å‘è€…èƒ½å¤Ÿæ›´å®¹æ˜“åœ°æ„å»ºå¤æ‚çš„åº”ç”¨ç¨‹åºã€‚\"\n",
    "\n",
    "splitter = CharacterTextSplitter(\n",
    "    chunk_size = 50,\n",
    "    chunk_overlap = 5,\n",
    "    separator= \"\"  #è®¾ç½®ä¸ºç©ºå­—ç¬¦æ—¶ï¼Œè¡¨ç¤ºç¦ç”¨åˆ†éš”ç¬¦\n",
    ")\n",
    "\n",
    "texts = splitter.split_text(text)\n",
    "\n",
    "for i,chunk in enumerate(texts):\n",
    "    print(f\"å—{i+1}:é•¿åº¦:{len(chunk)}\")\n",
    "    print(chunk)\n",
    "    print('-'*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a1e61f0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "å—1ï¼šé•¿åº¦:9\n",
      "è¿™æ˜¯ä¸€ä¸ªç¤ºä¾‹æ–‡æœ¬å•Š\n",
      "--------------------------------------------------\n",
      "å—2ï¼šé•¿åº¦:41\n",
      "æˆ‘ä»¬å°†ä½¿ç”¨CharacterTextSplitterå°†å…¶åˆ†å‰²æˆå°å—ã€‚åˆ†å‰²åŸºäºå­—ç¬¦æ•°\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from langchain_text_splitters import CharacterTextSplitter\n",
    "\n",
    "text = \"è¿™æ˜¯ä¸€ä¸ªç¤ºä¾‹æ–‡æœ¬å•Šã€‚æˆ‘ä»¬å°†ä½¿ç”¨CharacterTextSplitterå°†å…¶åˆ†å‰²æˆå°å—ã€‚åˆ†å‰²åŸºäºå­—ç¬¦æ•°ã€‚\"\n",
    "\n",
    "text_splitter = CharacterTextSplitter(\n",
    "    chunk_size = 42,\n",
    "    chunk_overlap = 5, #\n",
    "    separator=\"ã€‚\"   #æŒ‰å¥å·åˆ†å‰²ï¼ˆåˆ†éš”ç¬¦ä¼˜å…ˆï¼‰\n",
    ")\n",
    "\n",
    "chunks = text_splitter.split_text(text)\n",
    "\n",
    "for i,chunk in enumerate(chunks):\n",
    "    print(f\"å—{i+1}ï¼šé•¿åº¦:{len(chunk)}\")\n",
    "    print(chunk)\n",
    "    print(\"-\"*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a4013ee2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "å—:1é•¿åº¦ï¼š15\n",
      "è¿™æ˜¯ç¬¬ä¸€æ®µæ–‡æœ¬ã€‚è¿™æ˜¯ç¬¬äºŒæ®µå†…å®¹\n",
      "--------------------------------------------------\n",
      "å—:2é•¿åº¦ï¼š14\n",
      "è¿™æ˜¯ç¬¬äºŒæ®µå†…å®¹ã€‚æœ€åä¸€æ®µç»“æŸ\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "\n",
    "text = \"è¿™æ˜¯ç¬¬ä¸€æ®µæ–‡æœ¬ã€‚è¿™æ˜¯ç¬¬äºŒæ®µå†…å®¹ã€‚æœ€åä¸€æ®µç»“æŸã€‚\"\n",
    "\n",
    "text_splitter = CharacterTextSplitter(\n",
    "    separator=\"ã€‚\",\n",
    "    chunk_size = 20,\n",
    "    chunk_overlap = 8,\n",
    "    # keep_seperator = True #chunkä¸­æ˜¯å¦ä¿ç•™åˆ‡å‰²ç¬¦\n",
    ")\n",
    "\n",
    "chunks = text_splitter.split_text(text)\n",
    "\n",
    "for i,chunk in enumerate(chunks):\n",
    "    print(f\"å—:{i+1}é•¿åº¦ï¼š{len(chunk)}\")\n",
    "    print(chunk)\n",
    "    print(\"-\"*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4174e282",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LangChainæ¡†\n",
      "-------\n",
      "æ¶ç‰¹æ€§\n",
      "-------\n",
      "å¤šæ¨¡å‹é›†æˆ(GPT\n",
      "-------\n",
      "/Claude)\n",
      "-------\n",
      "è®°å¿†ç®¡ç†åŠŸèƒ½\n",
      "-------\n",
      "é“¾å¼è°ƒç”¨è®¾è®¡ã€‚æ–‡æ¡£\n",
      "-------\n",
      "åˆ†æåœºæ™¯ç¤ºä¾‹ï¼šéœ€è¦å¤„\n",
      "-------\n",
      "ç†PDF/Wordç­‰\n",
      "-------\n",
      "æ ¼å¼ã€‚\n",
      "-------\n"
     ]
    }
   ],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size = 10,\n",
    "    chunk_overlap = 0,\n",
    "    # add_start_index = True\n",
    ")\n",
    "\n",
    "text = \"LangChainæ¡†æ¶ç‰¹æ€§\\n\\nå¤šæ¨¡å‹é›†æˆ(GPT/Claude)\\nè®°å¿†ç®¡ç†åŠŸèƒ½\\né“¾å¼è°ƒç”¨è®¾è®¡ã€‚æ–‡æ¡£åˆ†æåœºæ™¯ç¤ºä¾‹ï¼šéœ€è¦å¤„ç†PDF/Wordç­‰æ ¼å¼ã€‚\"\n",
    "\n",
    "paragraphs = text_splitter.split_text(text)\n",
    "\n",
    "for para in paragraphs:\n",
    "    print(para)\n",
    "    print('-------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0b5e4818",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page_content='LangChainæ¡†' metadata={'start_index': 0}\n",
      "-------\n",
      "page_content='æ¶ç‰¹æ€§' metadata={'start_index': 10}\n",
      "-------\n",
      "page_content='å¤šæ¨¡å‹é›†æˆ(GPT' metadata={'start_index': 15}\n",
      "-------\n",
      "page_content='/Claude)' metadata={'start_index': 24}\n",
      "-------\n",
      "page_content='è®°å¿†ç®¡ç†åŠŸèƒ½' metadata={'start_index': 33}\n",
      "-------\n",
      "page_content='é“¾å¼è°ƒç”¨è®¾è®¡ã€‚æ–‡æ¡£' metadata={'start_index': 40}\n",
      "-------\n",
      "page_content='åˆ†æåœºæ™¯ç¤ºä¾‹ï¼šéœ€è¦å¤„' metadata={'start_index': 49}\n",
      "-------\n",
      "page_content='ç†PDF/Wordç­‰' metadata={'start_index': 59}\n",
      "-------\n",
      "page_content='æ ¼å¼ã€‚' metadata={'start_index': 69}\n",
      "-------\n"
     ]
    }
   ],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size = 10,\n",
    "    chunk_overlap = 0,\n",
    "    add_start_index = True\n",
    ")\n",
    "\n",
    "text_list = [\"LangChainæ¡†æ¶ç‰¹æ€§\\n\\nå¤šæ¨¡å‹é›†æˆ(GPT/Claude)\\nè®°å¿†ç®¡ç†åŠŸèƒ½\\né“¾å¼è°ƒç”¨è®¾è®¡ã€‚æ–‡æ¡£åˆ†æåœºæ™¯ç¤ºä¾‹ï¼šéœ€è¦å¤„ç†PDF/Wordç­‰æ ¼å¼ã€‚\"]\n",
    "\n",
    "\n",
    "\n",
    "paragraphs = text_splitter.create_documents(text_list)\n",
    "\n",
    "for para in paragraphs:\n",
    "    print(para)\n",
    "    print('-------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8fb04194",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'str'>\n",
      "ğŸ”¥å¤§æ¨¡å‹LLMçš„æ¶æ„ä»‹ç»ï¼Ÿ\n",
      "ğŸ”¥å¤§æ¨¡å‹LLMï¼ˆLarge Language\n",
      "ğŸ”¥Modelsï¼‰é€šå¸¸é‡‡ç”¨åŸºäºTransformerçš„æ¶æ„ã€‚Transformeræ¨¡å‹ç”±å¤šä¸ªç¼–ç å™¨æˆ–è§£ç å™¨å±‚ç»„æˆï¼Œæ¯ä¸ªå±‚åŒ…å«å¤šå¤´è‡ªæ³¨æ„åŠ›æœºåˆ¶å’Œå‰é¦ˆç¥ç»ç½‘ç»œã€‚è¿™äº›å±‚å¯ä»¥å¹¶è¡Œå¤„ç†è¾“å…¥åºåˆ—ä¸­çš„æ‰€æœ‰ä½ç½®ï¼Œæ•è·é•¿\n",
      "ğŸ”¥å¯ä»¥å¹¶è¡Œå¤„ç†è¾“å…¥åºåˆ—ä¸­çš„æ‰€æœ‰ä½ç½®ï¼Œæ•è·é•¿è·ç¦»ä¾èµ–å…³ç³»ã€‚å¤§æ¨¡å‹é€šå¸¸å…·æœ‰æ•°åäº¿ç”šè‡³æ•°åƒäº¿ä¸ªå‚æ•°ï¼Œå¯ä»¥å¤„ç†å¤§é‡çš„æ–‡æœ¬æ•°æ®ï¼Œå¹¶åœ¨å„ç§NLPä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ã€‚\n",
      "ğŸ”¥å‰é¦ˆç¥ç»ç½‘ç»œï¼ˆFeedforward Neural\n",
      "ğŸ”¥Networkï¼‰æ˜¯ä¸€ç§æœ€åŸºç¡€çš„ç¥ç»ç½‘ç»œç±»å‹ï¼Œå®ƒçš„ä¿¡æ¯æµåŠ¨æ˜¯å•å‘çš„ï¼Œä»è¾“å…¥å±‚ç»è¿‡ä¸€ä¸ªæˆ–å¤šä¸ªéšè—å±‚ï¼Œæœ€ç»ˆåˆ°è¾¾è¾“å‡ºå±‚ã€‚åœ¨å‰é¦ˆç¥ç»ç½‘ç»œä¸­ï¼Œç¥ç»å…ƒä¹‹é—´çš„è¿æ¥ä¸ä¼šå½¢æˆé—­ç¯ï¼Œè¿™æ„å‘³ç€ä¿¡å·åœ¨å‰å‘ä¼ æ’­è¿‡ç¨‹ä¸­ä¸ä¼šå›æº¯ã€‚\n",
      "ğŸ”¥å‰é¦ˆç¥ç»ç½‘ç»œçš„åŸºæœ¬ç»„æˆå•å…ƒæ˜¯ç¥ç»å…ƒï¼Œæ¯ä¸ªç¥ç»å…ƒéƒ½ä¼šå¯¹è¾“å…¥ä¿¡å·è¿›è¡ŒåŠ æƒæ±‚å’Œï¼Œç„¶åé€šè¿‡ä¸€ä¸ªæ¿€æ´»å‡½æ•°äº§ç”Ÿè¾“å‡ºã€‚æ¿€æ´»å‡½æ•°é€šå¸¸æ˜¯éçº¿æ€§çš„ï¼Œå®ƒå†³å®šäº†ç¥ç»å…ƒçš„è¾“å‡ºæ˜¯å¦åº”è¯¥è¢«æ¿€æ´»ï¼Œä»è€Œå…è®¸ç½‘ç»œå­¦ä¹ å¤æ‚å’Œéçº¿æ€§çš„å‡½æ•°\n",
      "ğŸ”¥æ¿€æ´»ï¼Œä»è€Œå…è®¸ç½‘ç»œå­¦ä¹ å¤æ‚å’Œéçº¿æ€§çš„å‡½æ•°ã€‚\n",
      "ğŸ”¥å‰é¦ˆç¥ç»ç½‘ç»œåœ¨æ¨¡å¼è¯†åˆ«ã€å‡½æ•°é€¼è¿‘ã€åˆ†ç±»ã€å›å½’ç­‰å¤šä¸ªé¢†åŸŸéƒ½æœ‰åº”ç”¨ã€‚ä¾‹å¦‚ï¼Œåœ¨å›¾åƒè¯†åˆ«ä»»åŠ¡ä¸­ï¼Œç½‘ç»œçš„è¾“å…¥å±‚èŠ‚ç‚¹å¯èƒ½å¯¹åº”äºå›¾åƒçš„åƒç´ å€¼ï¼Œè€Œè¾“å‡ºå±‚èŠ‚ç‚¹å¯èƒ½ä»£è¡¨ä¸åŒç±»åˆ«çš„æ¦‚ç‡åˆ†å¸ƒã€‚\n",
      "ğŸ”¥prefix LM å’Œ causal LMã€encoder-decoder åŒºåˆ«åŠå„è‡ªæœ‰ä»€ä¹ˆä¼˜ç¼ºç‚¹ï¼Ÿ\n",
      "ğŸ”¥prefix\n",
      "ğŸ”¥LMï¼šé€šè¿‡åœ¨è¾“å…¥åºåˆ—å‰æ·»åŠ å¯å­¦ä¹ çš„ä»»åŠ¡ç›¸å…³å‰ç¼€ï¼Œå¼•å¯¼æ¨¡å‹ç”Ÿæˆé€‚åº”ç‰¹å®šä»»åŠ¡çš„è¾“å‡ºã€‚ä¼˜ç‚¹æ˜¯å¯ä»¥å‡å°‘å¯¹é¢„è®­ç»ƒæ¨¡å‹å‚æ•°çš„ä¿®æ”¹ï¼Œé™ä½è¿‡æ‹Ÿåˆé£é™©ï¼›ç¼ºç‚¹æ˜¯å¯èƒ½å—åˆ°å‰ç¼€è¡¨ç¤ºé•¿åº¦çš„é™åˆ¶ï¼Œæ— æ³•å……åˆ†æ•æ‰ä»»åŠ¡ç›¸å…³çš„ä¿¡æ¯ã€‚\n",
      "ğŸ”¥causal LMï¼šæ ¹æ®ä¹‹å‰ç”Ÿæˆçš„ token é¢„æµ‹ä¸‹ä¸€ä¸ª tokenï¼Œå¯ä»¥ç”Ÿæˆè¿è´¯çš„æ–‡æœ¬ã€‚ä¼˜ç‚¹æ˜¯å¯ä»¥ç”Ÿæˆçµæ´»çš„æ–‡æœ¬ï¼Œé€‚åº”å„ç§ç”Ÿæˆä»»åŠ¡ï¼›ç¼ºç‚¹æ˜¯æ— æ³•è®¿é—®æœªæ¥çš„ä¿¡æ¯ï¼Œå¯èƒ½ç”Ÿæˆä¸ä¸€è‡´æˆ–æœ‰è¯¯çš„å†…å®¹ã€‚\n",
      "ğŸ”¥encoder-decoderï¼šç”±ç¼–ç å™¨å’Œè§£ç å™¨ç»„æˆï¼Œç¼–ç å™¨å°†è¾“å…¥åºåˆ—ç¼–ç ä¸ºå›ºå®šé•¿åº¦çš„å‘é‡ï¼Œè§£ç å™¨æ ¹æ®ç¼–ç å™¨çš„è¾“å‡ºç”Ÿæˆè¾“å‡ºåºåˆ—ã€‚ä¼˜ç‚¹æ˜¯å¯ä»¥å¤„ç†è¾“å…¥å’Œè¾“å‡ºåºåˆ—ä¸åŒé•¿åº¦çš„ä»»åŠ¡ï¼Œå¦‚æœºå™¨ç¿»è¯‘ï¼›ç¼ºç‚¹æ˜¯æ¨¡å‹ç»“æ„\n",
      "ğŸ”¥åŒé•¿åº¦çš„ä»»åŠ¡ï¼Œå¦‚æœºå™¨ç¿»è¯‘ï¼›ç¼ºç‚¹æ˜¯æ¨¡å‹ç»“æ„è¾ƒä¸ºå¤æ‚ï¼Œè®­ç»ƒå’Œæ¨ç†è®¡ç®—é‡è¾ƒå¤§ã€‚\n"
     ]
    }
   ],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "with open(\"/home/qixia/langchain/chapter07/a.txt\",encoding=\"utf-8\") as f:\n",
    "    state_of_the_union = f.read()\n",
    "\n",
    "print(type(state_of_the_union))\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size = 100,\n",
    "    chunk_overlap = 20,\n",
    "    length_function = len\n",
    ")\n",
    "\n",
    "texts = text_splitter.create_documents([state_of_the_union])\n",
    "\n",
    "for text in texts:\n",
    "    print(f\"ğŸ”¥{text.page_content}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c990223c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "åŸå§‹æ–‡æœ¬è¢«åˆ†å‰²æˆäº†3ä¸ªå—ï¼š\n",
      "å—1ï¼šé•¿åº¦ï¼š29å†…å®¹ï¼šäººå·¥æ™ºèƒ½æ˜¯ä¸€ä¸ªå¼ºå¤§çš„å¼€å‘æ¡†æ¶ã€‚å®ƒæ”¯æŒå¤šç§è¯­è¨€æ¨¡å‹å’Œå·¥å…·é“¾ã€‚\n",
      "--------------------------------------------------\n",
      "å—2ï¼šé•¿åº¦ï¼š32å†…å®¹ï¼šäººå·¥æ™ºèƒ½æ˜¯æŒ‡é€šè¿‡è®¡ç®—æœºç¨‹åºæ¨¡æ‹Ÿäººç±»æ™ºèƒ½çš„ä¸€é—¨ç§‘å­¦ã€‚è‡ª20ä¸–çºª50\n",
      "--------------------------------------------------\n",
      "å—3ï¼šé•¿åº¦ï¼š19å†…å®¹ï¼šå¹´ä»£è¯ç”Ÿä»¥æ¥ï¼Œäººå·¥æ™ºèƒ½ç»å†äº†å¤šæ¬¡èµ·ä¼ã€‚\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from langchain.text_splitter import TokenTextSplitter\n",
    "\n",
    "text_splitter = TokenTextSplitter(\n",
    "    chunk_size = 33,\n",
    "    chunk_overlap = 0,\n",
    "    encoding_name=\"cl100k_base\"\n",
    ")\n",
    "\n",
    "text = \"äººå·¥æ™ºèƒ½æ˜¯ä¸€ä¸ªå¼ºå¤§çš„å¼€å‘æ¡†æ¶ã€‚å®ƒæ”¯æŒå¤šç§è¯­è¨€æ¨¡å‹å’Œå·¥å…·é“¾ã€‚äººå·¥æ™ºèƒ½æ˜¯æŒ‡é€šè¿‡è®¡ç®—æœºç¨‹åºæ¨¡æ‹Ÿäººç±»æ™ºèƒ½çš„ä¸€é—¨ç§‘å­¦ã€‚è‡ª20ä¸–çºª50å¹´ä»£è¯ç”Ÿä»¥æ¥ï¼Œäººå·¥æ™ºèƒ½ç»å†äº†å¤šæ¬¡èµ·ä¼ã€‚\"\n",
    "\n",
    "texts = text_splitter.split_text(text)\n",
    "\n",
    "print(f\"åŸå§‹æ–‡æœ¬è¢«åˆ†å‰²æˆäº†{len(texts)}ä¸ªå—ï¼š\")\n",
    "for i,chunk in enumerate(texts):\n",
    "    print(f\"å—{i+1}ï¼šé•¿åº¦ï¼š{len(chunk)}å†…å®¹ï¼š{chunk}\")\n",
    "    print(\"-\"*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "aa0d032c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Created a chunk of size 25, which is longer than the specified 18\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "åˆ†å‰²åçš„å—æ•°ï¼š4\n",
      "å—1:17 Token\n",
      "å†…å®¹ï¼šäººå·¥æ™ºèƒ½æ˜¯ä¸€ä¸ªå¼ºå¤§çš„å¼€å‘æ¡†æ¶\n",
      "\n",
      "å—2:14 Token\n",
      "å†…å®¹ï¼šå®ƒæ”¯æŒå¤šç§è¯­è¨€æ¨¡å‹å’Œå·¥å…·é“¾\n",
      "\n",
      "å—3:25 Token\n",
      "å†…å®¹ï¼šäººå·¥æ™ºèƒ½æ˜¯æŒ‡é€šè¿‡è®¡ç®—æœºç¨‹åºæ¨¡æ‹Ÿäººç±»æ™ºèƒ½çš„ä¸€é—¨ç§‘å­¦\n",
      "\n",
      "å—4:28 Token\n",
      "å†…å®¹ï¼šè‡ª20ä¸–çºª50å¹´ä»£è¯ç”Ÿä»¥æ¥ï¼Œäººå·¥æ™ºèƒ½ç»å†äº†å¤šæ¬¡èµ·ä¼\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "import tiktoken\n",
    "\n",
    "text_splitter = CharacterTextSplitter.from_tiktoken_encoder(\n",
    "    encoding_name=\"cl100k_base\",\n",
    "    chunk_size = 18,\n",
    "    chunk_overlap=0,\n",
    "    separator=\"ã€‚\",\n",
    "    keep_separator = False\n",
    ")\n",
    "\n",
    "text = \"äººå·¥æ™ºèƒ½æ˜¯ä¸€ä¸ªå¼ºå¤§çš„å¼€å‘æ¡†æ¶ã€‚å®ƒæ”¯æŒå¤šç§è¯­è¨€æ¨¡å‹å’Œå·¥å…·é“¾ã€‚äººå·¥æ™ºèƒ½æ˜¯æŒ‡é€šè¿‡è®¡ç®—æœºç¨‹åºæ¨¡æ‹Ÿäººç±»æ™ºèƒ½çš„ä¸€é—¨ç§‘å­¦ã€‚è‡ª20ä¸–çºª50å¹´ä»£è¯ç”Ÿä»¥æ¥ï¼Œäººå·¥æ™ºèƒ½ç»å†äº†å¤šæ¬¡èµ·ä¼ã€‚\"\n",
    "\n",
    "texts = text_splitter.split_text(text)\n",
    "print(f\"åˆ†å‰²åçš„å—æ•°ï¼š{len(texts)}\")\n",
    "\n",
    "encoder = tiktoken.get_encoding(\"cl100k_base\")\n",
    "\n",
    "for i,chunk in enumerate(texts):\n",
    "    tokens = encoder.encode(chunk)\n",
    "    print(f\"å—{i+1}:{len(tokens)} Token\\nå†…å®¹ï¼š{chunk}\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
